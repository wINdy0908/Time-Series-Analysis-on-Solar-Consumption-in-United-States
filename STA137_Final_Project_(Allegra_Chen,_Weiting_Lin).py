# -*- coding: utf-8 -*-
"""STA137 Final Project (Allegra Chen, Weiting Lin).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-hF9IvYPdm441_9OTJtHqwLb76xm0VSA
"""

import openpyxl
from openpyxl import load_workbook
from datetime import datetime
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import re
from sklearn.linear_model import LinearRegression
import math
from scipy import stats
import scipy
import matplotlib.dates as mdate
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.stats.diagnostic import acorr_ljungbox
from sklearn.metrics import mean_squared_error, mean_absolute_error
import statsmodels.api as sm
import warnings

"""# Data Preprocessing"""

filepath="/content/Table_10.5__Solar_Energy_Consumption.xlsx"

# retrieve data
solar_data=[]
time_data=[]
data=pd.read_excel(filepath, sheet_name="Monthly Data")

# find the position of col of 'Solar Energy Consumption'
target_value = 'Solar Energy Consumption'
matching_rows_col = np.where(data.isin([target_value]))
target_col=int(matching_rows_col[1])

# find the position of 'Month'
time_value="Month"
matching_row_cols_time = np.where(data.isin([time_value]))
Month_col=int(matching_row_cols_time[1])
Month_row=int(matching_row_cols_time[0])

#retrieve target data frame
ts_data=data.iloc[Month_row:,[Month_col,target_col]].dropna()
ts_data=ts_data.reset_index(drop=True)
ts_data.columns=ts_data.iloc[0,:]
ts_data=ts_data.drop(0)


# na proportion
na_proportion=ts_data[ts_data["Solar Energy Consumption"]=="Not Available"].shape[0]/ts_data.shape[0]
print(f"na proportion in raw data:{na_proportion}")

#remove "Not Available" data
ts_data=ts_data[ts_data["Solar Energy Consumption"]!="Not Available"]

# set Month as index
ts_data=ts_data.set_index("Month")
ts_data.columns=["consumption"]

"""# EDA

1. Retrieve data starting from 2012, because the trend of solar energy consumption in previous years is not robust
2. Set 90% data as trainning data and remaining 10% data as testing data
"""

# describe statistics
ts_data["consumption"]=pd.to_numeric(ts_data["consumption"])
print(f"statistics information of data since 1984")
print(ts_data.describe())

# ts plot
plt.plot(ts_data.index, ts_data['consumption'])
plt.xlabel('Year Month')
plt.ylabel('Consumption (Trillion Btu)')
plt.title('Original Data')

# proportion of data after 2012
print(f"proportion of data after 2012:{ts_data[ts_data.index >= '2012-01-01'].shape[0]/ts_data.shape[0]}")
print(f"statistics information of data since 2012")
print(ts_data.describe())
print(f"final number of data is {ts_data.shape[0]}")

plt.plot(ts_data.index, ts_data['consumption'])
plt.xlabel('Year Month')
plt.ylabel('Consumption (Trillion Btu)')
plt.axvline(pd.Timestamp('2012-01-01'), color='r', linestyle='-',label="2012-01")
plt.legend()
plt.title('Original Data')

"""Select data from after 2012"""

ts_data=ts_data[ts_data.index >= '2012-01-01']

# plot trimmed data
plt.plot(ts_data.index, ts_data['consumption'])
plt.xlabel('Year Month')
plt.ylabel('Consumption (Trillion Btu)')
plt.title('Trimmed Data')

# split data into trainning data and testing data
split_index = math.ceil(len(ts_data)*0.9)
print(f"number of trainning data after 2012 is {split_index}")
print(f"number of testing data after 2012 is {len(ts_data)-split_index}")
training_data=ts_data.iloc[:split_index,:]
testing_data=ts_data.iloc[split_index:,:]

training_data.head(3)

testing_data.head(3)

"""# 2. Data Analysis

We notice that the variance increase.
## 2.1. Transformation method

### 2.1.1 Box-Cox transformation
"""

consumption, fitted_lambda = stats.boxcox(training_data['consumption'])
# training_data['consumption'] = consumption
plt.plot(training_data.index, consumption)
plt.xlabel('Year Month')
plt.ylabel('Consumption (Trillion Btu)')
plt.title("Training data after Box-Cox transformation")

"""### 2.1.2 Log transformation"""

plt.plot(np.log(training_data['consumption']))
plt.xlabel('Year Month')
plt.ylabel('Consumption (Trillion Btu)')
plt.title("Training data after Log transformation")

"""### 2.1.3 Selecting transformation method
Log transformation better than Box-Cox due to the range of y-axis (consumption value)
"""

training_data['consumption'] = np.log(training_data['consumption'])

"""## 2.2. Analyzing the “smooth” component

According to the box cox transformed plot above, there is clearly pattern for trend and seasonality, i.e. yt = mt + st + xt.

### 2.2.1. Moving average methods
"""

# step 1: detrend
weights = np.array([.5])
weights = np.append(weights, np.repeat(1, 11))
weights = np.append(weights, .5)

def weighted_average(window):
  if len(window) != len(weights):
    return np.nan
  return (window * weights).sum() / 12

training_data['moving_avg'] = training_data['consumption'].rolling(window=13, center=True).apply(weighted_average, raw=True)

# Plot the original and moving average for comparison
# plt.figure(figsize=(10, 6))
plt.plot(training_data['consumption'], label='Original')
plt.plot(training_data['moving_avg'], label='Moving Average', color='red')
plt.xlabel('Year Month')
plt.ylabel('Log Transformed ')
plt.legend()
plt.show()

# Subtract the moving average from the original data
training_data['detrended'] = training_data['consumption'] - training_data['moving_avg']

# Plot the detrended data
plt.plot(training_data['detrended'], label='Detrended by Moving Average')
plt.legend()
plt.xlabel('Year Month')
plt.ylabel('Log Consumption')
plt.show()

training_data.tail(10)

# step 2: deseasonalize

# Map the seasonal component to each corresponding month in the original dataframe
training_data['month'] = training_data.index.month
seasonal_component = training_data.groupby('month')['detrended'].mean()
training_data['seasonal'] = training_data['month'].map(seasonal_component)

# Plot to see the seasonal component over time
plt.plot(training_data['seasonal'], label='Seasonal Component')
plt.legend()
plt.xlabel('Year Month')
plt.ylabel('Log Consumption')
plt.show()

# step 3: detrend again
import statsmodels.api as sm

deseason = training_data['consumption'] - training_data['seasonal']

plt.plot(deseason)
plt.ylabel('Log Consumption')
plt.xlabel('Year Month')
plt.show()

# Define predictor and response variables
x = np.arange(len(training_data))
y = deseason
# Create a range of degrees to test
degrees = range(1, 10)

# Initialize an empty list to store the AIC values
aic_values = []

# Loop over the degrees
for d in degrees:
    # Fit a polynomial regression model of degree d
    model = sm.OLS(y, sm.add_constant(np.column_stack([x**i for i in range(1, d+1)]))).fit()
    # Append the AIC value to the list
    aic_values.append(model.aic)
    # Print the model summary
    print(f"Degree {d}:")
    print(model.summary())
    print()

# Find the index of the minimum AIC value
min_index = np.argmin(aic_values)

# Print the optimal degree
print(f"The optimal degree is {degrees[min_index]} with AIC = {aic_values[min_index]}")

# Plot the AIC values vs the degrees
plt.plot(degrees, aic_values, marker='o')
plt.xlabel('Degree')
plt.ylabel('AIC')
plt.show()

# Combining results of AIC, adj. R-squred, and p-value of the hypothesis test for beta coefficient=0,
# we pick the degree = 4
design_mat = sm.add_constant(np.column_stack([x**i for i in range(1, 5)]))
poly4model = sm.OLS(y, design_mat).fit()
trend_est = poly4model.predict(design_mat)

# Plot the data and the fitted model of the optimal degree
plt.scatter(x, y)
plt.plot(x, trend_est, color='red')
plt.xlabel('x')
plt.ylabel('y')
plt.show()

training_data['residual'] = deseason - trend_est
plt.plot(training_data['residual'])
plt.title('residuals')

"""### 2.2.1. ADF Test for stationarity"""

from statsmodels.tsa.stattools import adfuller

result = adfuller(training_data['residual'].dropna())
print('ADF Statistic:', result[0])
print('p-value:', result[1])

# Interpretation
if result[1] > 0.05:
    print("Possibly significant trend (Fail to reject H0)")
else:
    print("No significant trend (Reject H0)")

"""### 2.2.2 Check mean and variance of residuals group by year"""

annual_mean = training_data['residual'].resample('Y').mean()
annual_variance = training_data['residual'].resample('Y').var()

# Plotting the results
plt.figure(figsize=(12, 6))

plt.subplot(2, 1, 1)
plt.plot(annual_mean, label='Annual Mean')
plt.title('Annual mean of the residuals')
plt.legend()

plt.subplot(2, 1, 2)
plt.plot(annual_variance, label='Annual Variance')
plt.title('Annual variance of the residuals')
plt.legend()

plt.tight_layout()
plt.show()

"""## 2.3. Assessing residuals

According the results from the sample ACF plot, we can see there are definitely way more than 5% of the ACF values that are outside the 95% confidence interval.

These indicate that the residuals are not independent and we need to fit some models for the residuals further.
"""

import statsmodels.api as sm
fig, ax = plt.subplots()
sm.graphics.tsa.plot_acf(training_data['residual'].dropna(), bartlett_confint=False,lags=50,ax=ax)
ax.set_xlabel("Lags")
ax.set_ylabel("Autocorrelation value")
ax.set_title("Autocorrelation plot of stationary residuals.")
plt.show()

# Check normality: Shapiro-Wilk Test (p-value>0.05) do not reject H0: data is not significantly different from normal.
shapiro_test = stats.shapiro(training_data['residual'].dropna())
if shapiro_test[1] > 0.05:
  print("residuals are not significantly different from normal.")
else:
  print("residuals are significantly different from normal.")

# Check dependency:L-jung Box (p-value>0.05) H0: data is not significant dependent
L_jung_box=acorr_ljungbox(training_data['residual'].dropna(),return_df=True)
plt.bar(L_jung_box.index,L_jung_box["lb_pvalue"])
plt.xlabel("Lags")
plt.ylabel("p-value (1e-9/unit)")
plt.title("L-jung Box of stationary residuals")

"""## 2.4. Analyzing the “rough” component

From the ACF plot above, the ACFs tail off as lags increase. This suggests that we could try an AR or a ARMA model. Next we inspect the PACF plot to further assit our decison on whether the model should be a simple AR or ARMA. It looks like we can try with AR(8) and ARMA(p, q) p=1,...,8, q=0,..,3
"""

from statsmodels.graphics.tsaplots import plot_acf

import matplotlib.pyplot as plt
from statsmodels.graphics.tsaplots import plot_pacf
from statsmodels.tsa.stattools import pacf

fig, ax = plt.subplots()
plot_pacf(training_data['residual'], ax=ax, lags=20)

# Calculate PACF values
pacf_values = pacf(training_data['residual'], nlags=20)

# Label specific lags
lags_to_label = [1,9,10,14]

for lag in lags_to_label:
    pacf_value = pacf_values[lag]
    if pacf_value>=0:
      ax.annotate(f'{lag}', xy=(lag, pacf_value), xytext=(lag, pacf_value + 0.05),
                ha='center')  # Horizontal alignment
    else:
      ax.annotate(f'{lag}', xy=(lag, pacf_value), xytext=(lag, pacf_value - 0.1),
                ha='center')

ax.set_xlabel("Lags")
ax.set_ylabel("Partial autocorrelation value")
ax.set_title("Partial autocorrelation plot of stationary residuals")

plt.show()

"""### 2.4.1. Proposed AR model

AR(1) has the smallest AIC of -434.822
"""

aic_values = []
lags = [1, 9, 10, 14]

with warnings.catch_warnings():
    warnings.simplefilter("ignore")
    for lag in lags:
      model = sm.tsa.ARIMA(training_data['residual'].dropna(), order=(lag, 0,0))
      model_fit = model.fit()
      aic_values.append(model_fit.aic)

with warnings.catch_warnings():
    warnings.simplefilter("ignore")
    best_lag = lags[aic_values.index(min(aic_values))]
    model = sm.tsa.ARIMA(training_data['residual'].dropna(), order=( best_lag, 0,0))
    result = model.fit()
    print(result.summary())

# plot AIC value of AR(1), AR(9), AR(10), AR(14)
plt.plot(range(1,5),aic_values)
plt.xticks(ticks=[1, 2, 3,4], labels=['AR(1)', 'AR(9)', 'AR(10)', 'AR(14)'])
plt.title("AIC of AR model at different order")
plt.xlabel("order")
plt.ylabel("value")

# Test for residual independence
L_jung_box=acorr_ljungbox(result.resid.dropna(),return_df=True)
L_jung_box
plt.bar(L_jung_box.index,L_jung_box["lb_pvalue"])
plt.xlabel("lags")
plt.ylabel("p-value")
plt.axhline(y=0.05, color='r', linestyle='-',label="p-value=0.05")
plt.legend()
plt.title("L-jung Box of residuals after fitting AR(1) model")

# plot acf of residuals from AR(1) model
fig, ax = plt.subplots()
plot_acf(result.resid.dropna(), ax=ax, lags=20,bartlett_confint=False)
ax.set_title("Autocorrelation plot of residuals from AR(1)")
ax.set_xlabel("Lags")
ax.set_ylabel("Autocorrelation value")

plt.show()

"""### 2.4.2. Proposed models ARMA(p, q) where p=1,...,5, q=1,..5

1. based on Prob(Q), we learn that the p-vlaue is 0.81 and it exceeds 0.05, there is no dependence in residual.
"""

import warnings

def best_arma(p,q):
  aic=np.zeros(shape=(p+1,q+1))
  for p in range(1,p+1):
    for q in range(1,q+1):
      # Fit ARMA model with identified orders
      with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        model = sm.tsa.ARIMA(training_data['residual'].dropna(), order=(p, 0,q))
        fitted_model = model.fit()
        aic[p, q] = fitted_model.aic

  pq_min = np.unravel_index(aic.argmin(), aic.shape)
  best_p=pq_min[0]
  best_q=pq_min[1]

  with warnings.catch_warnings():
    warnings.simplefilter("ignore")
    model = sm.tsa.ARIMA(training_data['residual'].dropna(), order=(best_p, 0, best_q))
    fitted_model = model.fit()
    print(fitted_model.summary())
    residuals = fitted_model.resid
    print(residuals.describe())

  L_jung_box=acorr_ljungbox(residuals.dropna(),return_df=True)
  L_jung_box

  # plot L_jung_box
  plt.bar(L_jung_box.index,L_jung_box["lb_pvalue"])
  plt.xlabel("lags")
  plt.ylabel("p-value")
  plt.axhline(y=0.05, color='r', linestyle='-',label="p-value=0.05")
  plt.legend()
  plt.title(f"L-jung Box of residuals from ARMA({best_p},{best_q}) model")
  plt.show()

  # plot residual of ARMA model
  plt.plot(fitted_model.resid)
  plt.title(f"residuals of ARMA({best_p}, {best_q})")
  plt.show()

  # plot ACF of residuals from ARMA model
  fig, ax = plt.subplots()
  sm.graphics.tsa.plot_acf(fitted_model.resid, bartlett_confint=False)
  plot_acf(result.resid.dropna(), ax=ax, lags=20,bartlett_confint=False)
  ax.set_title(f"Autocorrelation plot of residuals from ARMA({best_p},{best_q})")
  ax.set_xlabel("Lags")
  ax.set_ylabel("Autocorrelation value")

  plt.show()
  print(aic)
  return best_p, best_q

best_p, best_q=best_arma(5,5)

"""### 2.4.3. Model Selection

ARMA(2,3) has a smaller AIC=-436.4 than AR(1) AIC=-434.822, we pick ARMA.

## 2.5. Predict future values

## 2.5.1. Forecast future Log transformed solar energy



1.  Use poly4model to predict future trend
2.  Use the seasonal component aligned with the corresponding future month.
3.  Use np.exp() to reverse the value, which is the sum of the value from the ARMA(2,3) model, future trend, and seasonal component.

consumption

y = m + s + x

### (a) Future trend estimation
"""

forecast_x=np.arange(len(training_data)+1,len(training_data)+1+len(testing_data))
design_mat = sm.add_constant(np.column_stack([forecast_x**i for i in range(1, 5)]))
trend_est = poly4model.predict(design_mat)

"""### (b) Sum up value from ARMA(2,3), trend estimation, and seasonal component"""

with warnings.catch_warnings():
    warnings.simplefilter("ignore")
    model = sm.tsa.ARIMA(training_data['residual'].dropna(), order=(best_p, 0,best_q))
    fitted_model = model.fit()
    forecast = fitted_model.forecast(steps=testing_data.shape[0])
    print(forecast)
    ## 2102
    y = forecast['2022-07-01':] + np.array(training_data.loc['2012-07-01':'2013-08-01', 'seasonal']) + trend_est
    ## 2017
    #y =  forecast+ np.array(training_data.loc['2018-01-01':'2018-08-01', 'seasonal']) + trend_est

"""### (c) Transform back the prediction by np.exp(y)"""

forecasted_consumption = np.exp(y)

plt.plot(ts_data, label='Observed Data')
plt.plot(forecasted_consumption, label = 'Testing Data Forecast')
plt.xlabel('Year Month')
plt.ylabel('Consumption (Trillion Btu)')
plt.legend()
plt.show()

"""### 2.5.2. Check Prediction Accuracy"""

y_true = ts_data.loc["2022-07-01":,"consumption"]  #2012
#y_true = ts_data.loc["2023-01-01":,"consumption"] #2017
y_pred = forecasted_consumption
print(y_pred .shape[0])

mae = mean_absolute_error(y_true, y_pred)
mse = mean_squared_error(y_true, y_pred)
rmse = np.sqrt(mse)
accuracy={"mae":mae,"mse":mse,"rmse":rmse}

print(f"MAE: {mae}")
print(f"MSE: {mse}")
print(f"RMSE: {rmse}")

plt.bar(accuracy.keys(),accuracy.values())
plt.xlabel("statistical metrics methods")
plt.ylabel("value")
plt.title("Forecasting accuracy")
plt.show()

# Adding a constant for the intercept
X = sm.add_constant(y_pred)
model = sm.OLS(y_true, X)
results = model.fit()

print("R-squared:", results.rsquared)

"""### 2.5.3. 2030 solar energy consumption prediction"""

# create sesonal component from 2022-07 to 2030-12
a=training_data.loc["2012-07-01":"2012-12-01","seasonal"]
b=training_data.loc["2012-01-01":"2012-12-01","seasonal"]
b=np.repeat(b,8)
seasonal_2030=np.append(a,b)

# forecaste trend from 2022-07 to 2030-12
forecast_x=np.arange(len(training_data)+1,len(training_data)+1+102)
design_mat = sm.add_constant(np.column_stack([forecast_x**i for i in range(1, 5)]))
trend_est = poly4model.predict(design_mat)

# forecaste residuals from 2022-07 to 2030-12
forecast = fitted_model.forecast(steps=102)

y=forecast+trend_est+seasonal_2030

future_val=np.exp(y)

plt.plot(future_val[:"2030-01-01"],label="forecasting value")
plt.plot(testing_data['consumption'],label="true value")
plt.legend()
plt.title("Solar energy consumption from 2022-07 to 2030-01")
plt.xlabel("year")
plt.ylabel("solar energy consumption")

est_2030_01=future_val["2030-01-01"]

print(f"forcasted value for 2030-01 is {est_2030_01}")

times=future_val["2030-01-01"]/future_val["2023-01-01"]

print(f"the estimated solar energy consumption in 2023 is {times} times than solar energy consumption in 2023 ")